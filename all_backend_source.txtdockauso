./app/modules/crawling.py
'''
from bs4 import BeautifulSoup
import requests
from typing import List, Dict
from urllib.parse import urlparse, urljoin, urlunparse

def clean_html(soup: BeautifulSoup) -> str:
    """
    ë¶ˆí•„ìš”í•œ ì†ì„±(style, class, id ë“±) ì œê±° í›„ HTML ë¬¸ìì—´ë¡œ ë°˜í™˜
    """
    # 1ï¸âƒ£ ë¶ˆí•„ìš”í•œ íƒœê·¸ ìì²´ ì œê±°
    for tag in soup(["script", "style", "noscript", "iframe"]):
        tag.decompose()

    # 2ï¸âƒ£ ê° íƒœê·¸ì˜ ë¶ˆí•„ìš”í•œ ì†ì„± ì œê±°
    for tag in soup.find_all(True):  # True â†’ ëª¨ë“  íƒœê·¸
        allowed_attrs = {"href", "src", "alt"}  # ìœ ì§€í•  ì†ì„±
        attrs = dict(tag.attrs)
        for attr in list(attrs.keys()):
            if attr not in allowed_attrs:
                del tag.attrs[attr]

    # 3ï¸âƒ£ ì •ëˆëœ HTML ë°˜í™˜
    return str(soup)

def normalize_url(base_url: str, link: str) -> str:
    """
    ìƒëŒ€ê²½ë¡œ â†’ ì ˆëŒ€ê²½ë¡œ ë³€í™˜ í›„, ì¤‘ë³µëœ path êµ¬ê°„ ì •ë¦¬
    ì˜ˆ: /ko/news/notice/ko/news/notice/5858 â†’ /ko/news/notice/5858
    """
    # 1ï¸âƒ£ ì ˆëŒ€ URLë¡œ ë³€í™˜
    full_url = urljoin(base_url, link)

    # 2ï¸âƒ£ URL íŒŒì‹±
    parsed = urlparse(full_url)
    path_parts = [p for p in parsed.path.split('/') if p]

    # 3ï¸âƒ£ ì¤‘ë³µëœ ì—°ì† íŒ¨í„´ ì œê±°
    cleaned = []
    for part in path_parts:
        # ê°™ì€ êµ¬ê°„ì´ ì—°ì†ìœ¼ë¡œ ë°˜ë³µë˜ë©´ í•˜ë‚˜ë§Œ ìœ ì§€
        if len(cleaned) >= 2 and cleaned[-2:] == [part, part]:
            continue
        cleaned.append(part)

    # 4ï¸âƒ£ ì¤‘ë³µ êµ¬ê°„ (ex. /ko/news/notice/ko/news/notice/5858)
    #    ê°™ì€ íŒ¨í„´ ë°˜ë³µì‹œ ì•ë¶€ë¶„ë§Œ ìœ ì§€
    joined = '/'.join(cleaned)
    while True:
        half = joined[: len(joined)//2]
        if half and joined.startswith(half + '/' + half):
            joined = joined[len(half)+1:]
        else:
            break

    new_path = '/' + joined

    # 5ï¸âƒ£ ë‹¤ì‹œ ì¡°ë¦½
    normalized = urlunparse((
        parsed.scheme,
        parsed.netloc,
        new_path,
        parsed.params,
        parsed.query,
        parsed.fragment,
    ))
    return normalized


def extract_links(url: str, selector: str, tag: str = "a", attr: str = "href") -> List[str]:
    """
    ì£¼ì–´ì§„ URLì—ì„œ íŠ¹ì • selector í•˜ìœ„ì˜ tagì—ì„œ attr ì†ì„±ë“¤ì„ ì¶”ì¶œ
    
    Args:
        url (str): í¬ë¡¤ë§í•  í˜ì´ì§€ URL
        selector (str): CSS selector (ì˜ˆ: "div.company-news", "div.news-list")
        tag (str): ì¶”ì¶œí•  íƒœê·¸ ì´ë¦„ (ê¸°ë³¸ê°’ "a")
        attr (str): ì¶”ì¶œí•  ì†ì„± (ê¸°ë³¸ê°’ "href")
    
    Returns:
        List[str]: ì¶”ì¶œëœ (ì •ê·œí™”ëœ) URL ë¦¬ìŠ¤íŠ¸
    """
    response = requests.get(url)
    response.raise_for_status()
    soup = BeautifulSoup(response.text, "html.parser")

    elements = soup.select(selector + f" {tag}")
    if not elements:
        raise ValueError(f"âŒ extract_links: '{selector} {tag}' selectorë¡œ ë§¤ì¹­ëœ ìš”ì†Œê°€ ì—†ìŠµë‹ˆë‹¤. ({url})")

    results = []
    for el in elements:
        raw_link = el.get(attr)
        if not raw_link:
            continue
        normalized = normalize_url(url, raw_link)
        results.append(normalized)

    if not results:
        raise ValueError(f"âŒ extract_links: '{attr}' ì†ì„±ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ({url})")

    return list(set(results))  # âœ… ì¤‘ë³µ ì œê±°


def get_contents(url: str, selector: str) -> Dict[str, List[Dict[str, str]]]:
    """
    ì§€ì •ëœ CSS selectorë¡œ ë³¸ë¬¸(html + ì´ë¯¸ì§€) ì¶”ì¶œ (ìŠ¤íƒ€ì¼ ì œê±° ë²„ì „)
    """
    response = requests.get(url)
    response.raise_for_status()
    soup = BeautifulSoup(response.text, "html.parser")

    sections = soup.select(selector)
    if not sections:
        raise ValueError(f"âŒ get_contents: selector '{selector}' ë¡œ ë§¤ì¹­ëœ ìš”ì†Œê°€ ì—†ìŠµë‹ˆë‹¤. ({url})")

    all_texts, all_images = [], []

    for section in sections:
        # ì´ë¯¸ì§€ ì¶”ì¶œ
        for img in section.find_all("img"):
            img_info = {"src": img.get("src"), "alt": img.get("alt", "")}
            all_images.append(img_info)

        # âœ… ìŠ¤íƒ€ì¼/í´ë˜ìŠ¤ ë“± ì†ì„± ì œê±°
        clean_section_html = clean_html(section)

        if clean_section_html.strip():
            all_texts.append(clean_section_html)

    text_with_tags = "\n".join(all_texts).strip()
    if not text_with_tags:
        raise ValueError(f"âŒ get_contents: selector '{selector}' ë‚´ë¶€ì—ì„œ ë³¸ë¬¸ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ({url})")

    return {"html": text_with_tags, "images": all_images}
'''

./app/modules/prompt_loader.py
'''
import os

PROMPT_DIR = os.path.join(os.path.dirname(__file__), "prompts")

def load_prompt(name: str) -> str:
    """
    ì§€ì •ëœ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ íŒŒì¼ì„ ë¡œë“œ
    Args:
        name (str): íŒŒì¼ëª… (í™•ì¥ì ì œì™¸)
    Returns:
        str: í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´
    """
    path = os.path.join(PROMPT_DIR, f"{name}.txt")
    if not os.path.exists(path):
        raise FileNotFoundError(f"Prompt file not found: {path}")
    with open(path, "r", encoding="utf-8") as f:
        return f.read()


'''

./app/modules/name_mapper.py
'''
import os

NAME_MAP_FILE = os.path.join(os.path.dirname(__file__), "prompts", "name_map.txt")

def load_name_map_text() -> str:
    """
    name_map.txt íŒŒì¼ì˜ ì›ë¬¸ì„ ê·¸ëŒ€ë¡œ ë¶ˆëŸ¬ì˜´.
    (í•œê¸€ëª…|ì˜ë¬¸ëª…|ê°„ë‹¨ì„¤ëª…)
    """
    if not os.path.exists(NAME_MAP_FILE):
        return ""
    with open(NAME_MAP_FILE, "r", encoding="utf-8") as f:
        return f.read().strip()


def append_name_entry(entry_line: str):
    """
    ìƒˆ í•­ëª©(í•œê¸€ëª…|ì˜ë¬¸ëª…|ê°„ë‹¨ì„¤ëª…)ì„ íŒŒì¼ ë§¨ ì•„ë˜ì— ì¶”ê°€
    """
    with open(NAME_MAP_FILE, "a", encoding="utf-8") as f:
        f.write("\n" + entry_line.strip())


def overwrite_name_map(new_text: str):
    """
    ì „ì²´ ë‚´ìš©ì„ ë®ì–´ì“°ê¸° (íŒŒì¼ êµì²´)
    """
    with open(NAME_MAP_FILE, "w", encoding="utf-8") as f:
        f.write(new_text.strip())


def delete_name_entry(korean_name: str):
    """
    íŠ¹ì • í•œê¸€ëª…ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ë¼ì¸ ì‚­ì œ
    (í•œê¸€ëª…|... í˜•íƒœì—ì„œ ì²« ë²ˆì§¸ êµ¬ë¶„ì ì „ê¹Œì§€ ì¼ì¹˜í•˜ëŠ” ë¼ì¸ì„ ì œê±°)
    """
    if not os.path.exists(NAME_MAP_FILE):
        raise FileNotFoundError("name_map.txt not found")

    with open(NAME_MAP_FILE, "r", encoding="utf-8") as f:
        lines = f.readlines()

    new_lines = []
    for line in lines:
        stripped = line.strip()
        if not stripped:
            continue
        if not stripped.split("|", 1)[0] == korean_name:
            new_lines.append(stripped)

    with open(NAME_MAP_FILE, "w", encoding="utf-8") as f:
        f.write("\n".join(new_lines))

'''

./app/modules/prompts/name_map.txt
'''
ì´ë³‘í—Œ|Lee Byung-hun|BH Entertainment
í•œíš¨ì£¼|Han Hyo-joo|BH Entertainment
ê¹€ê³ ì€|Kim Go-eun|BH Entertainment
ë°•ë³´ì˜|Park Bo-young|BH Entertainment
í•œì§€ë¯¼|Han Ji-min|BH Entertainment

ê³µìœ |Gong Yoo|Management SOOP
ê³µíš¨ì§„|Gong Hyo-jin|Management SOOP
ë°°ìˆ˜ì§€|Bae Suzy|Management SOOP
ìµœìš°ì‹|Choi Woo-shik|Management SOOP
ì •ìœ ë¯¸|Jung Yu-mi|Management SOOP
ë‚¨ì£¼í˜|Nam Joo-hyuk|Management SOOP

ì´ì •ì¬|Lee Jung-jae|Artist Company
ì •ìš°ì„±|Jung Woo-sung|Artist Company
ê¹€ìœ¤ì„|Kim Yoon-seok|Artist Company

í˜„ë¹ˆ|Hyun Bin|VAST Entertainment

ì†ì˜ˆì§„|Son Ye-jin|MSTEAM Entertainment
ì´ë¯¼ì •|Lee Min-jung|MSTEAM Entertainment
ìœ„í•˜ì¤€|Wi Ha-jun|MSTEAM Entertainment

ì‹ ë¯¼ì•„|Shin Min-a|AM Entertainment
ê¹€ìš°ë¹ˆ|Kim Woo-bin|AM Entertainment

ê¹€ìˆ˜í˜„|Kim Soo-hyun|GOLDMEDALIST

ì†¡ì¤‘ê¸°|Song Joong-ki|HighZium Studio
ê¹€ì§€ì›|Kim Ji-won|HighZium Studio

ì „ì§€í˜„|Jun Ji-hyun (Gianna Jun)|IEUM HASHTAG

ì†¡í˜œêµ|Song Hye-kyo|UAA (United Artist Agency)

ë°•ì‹ í˜œ|Park Shin-hye|SALT Entertainment
ê¹€ì„ í˜¸|Kim Seon-ho|SALT Entertainment

ë°•ì€ë¹ˆ|Park Eun-bin|Namoo Actors
ë¬¸ê·¼ì˜|Moon Geun-young|Namoo Actors

ë³€ìš”í•œ|Byun Yo-han|Saram Entertainment
ë°•ê·œì˜|Park Gyu-young|Saram Entertainment

ê¹€íƒœí¬|Kim Tae-hee|STORY J Company
ì„œì¸êµ­|Seo In-guk|STORY J Company

ì´ì œí›ˆ|Lee Je-hoon|COMPANY ON

ë¥˜ì¤€ì—´|Ryu Jun-yeol|C-JeS Studio

ë°•ë³´ê²€|Park Bo-gum|THEBLACKLABEL
'''

./app/modules/prompts/generate_news.txt
'''
Below is the original news article.

Refer to the following name mapping for translation consistency:
-------------------------
{{name_map}}
-------------------------




Original article content:
-------------------------
{{content}}
-------------------------

Translate the content into English and summarize it in the style of a professional news article.

Write the result so that it:
- Reads naturally as a native English news report
- Uses clear paragraph separation with **two line breaks between paragraphs**

Do NOT include any <img> or <caption> tags â€” the system will handle that automatically.

Output your final result strictly in the following XML structure:

<Title>English headline</Title>

<Article>
[Main article body â€” written in English, with paragraph breaks using two line spaces]
</Article>

Do NOT include explanations or commentary.
Output ONLY the two XML tags <Title> and <Article>.

'''

./app/modules/bedrock.py
'''
import boto3
import json
import re

# âœ… Bedrock í´ë¼ì´ì–¸íŠ¸
client = boto3.client(
    service_name="bedrock-runtime",
    region_name="us-east-1"
)

model_ids = {
    'haiku-3.5': 'arn:aws:bedrock:us-east-1:678005315499:inference-profile/us.anthropic.claude-3-5-haiku-20241022-v1:0'
}


def call_bedrock_api(prompt: str, model_name: str = 'haiku-3.5'):
    """
    Bedrock Claude 3.5 API í˜¸ì¶œ
    """
    response = client.invoke_model(
        modelId=model_ids[model_name],
        body=json.dumps({
            "anthropic_version": "bedrock-2023-05-31",
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": 1000,
            "temperature": 0.7,
        }),
        contentType="application/json",
        accept="application/json"
    )
    result = json.loads(response["body"].read())
    return result


def parse_bedrock_output(text: str):
    """
    Claude ì¶œë ¥ì—ì„œ <Title> / <Article> íƒœê·¸ ì¶”ì¶œ
    """
    title_match = re.search(r"<Title>(.*?)</Title>", text, re.DOTALL)
    article_match = re.search(r"<Article>(.*?)</Article>", text, re.DOTALL)
    title = title_match.group(1).strip() if title_match else ""
    article = article_match.group(1).strip() if article_match else ""
    return title, article



'''

./app/main.py
'''
# app.py
from fastapi import FastAPI, Query
from pydantic import BaseModel
from typing import List, Optional
from fastapi.middleware.cors import CORSMiddleware

# ëª¨ë“ˆ import
from app.modules.bedrock import call_bedrock_api
from app.modules.crawling import get_contents


from app.routes.news import router as news_router
from app.routes.source import router as source_router
from app.routes.articles import router as articles_router
from app.routes.scrap import router as scrap_router
from app.routes.name_map import router as name_router



app = FastAPI(title="My API Service", version="1.0.0")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],             
    allow_credentials=True,
    allow_methods=["*"],             
    allow_headers=["*"],             
)

app.include_router(news_router)
app.include_router(source_router)
app.include_router(articles_router)
app.include_router(scrap_router)
app.include_router(name_router)


# -------------------------------
# Request/Response ëª¨ë¸ ì •ì˜
# -------------------------------

class BedrockRequest(BaseModel):
    prompt: str
    messages: Optional[str] = None
    model_name: str = "haiku-3.5"


class BedrockResponse(BaseModel):
    output: str
    raw: dict


class CrawlResponse(BaseModel):
    html: str
    images: List[dict]


# -------------------------------
# ì—”ë“œí¬ì¸íŠ¸ ì •ì˜
# -------------------------------

@app.get("/")
def root():
    return {"message": "API is running!"}


@app.post("/bedrock", response_model=BedrockResponse)
def run_bedrock(req: BedrockRequest):
    """Bedrock ëª¨ë¸ í˜¸ì¶œ"""
    result = call_bedrock_api(
        prompt=req.prompt,
        messages=req.messages or "",
        model_name=req.model_name,
    )

    # Claude ê³„ì—´ ì‘ë‹µ íŒŒì‹±
    try:
        output_text = result["content"][0]["text"]
    except Exception:
        output_text = str(result)

    return BedrockResponse(output=output_text, raw=result)


@app.get("/crawl", response_model=CrawlResponse)
def crawl_url(url: str = Query(..., description="í¬ë¡¤ë§í•  URL"),
              selector: str = Query("sm-section-inner", description="div selector class")):
    """ì›¹í˜ì´ì§€ì—ì„œ ì½˜í…ì¸  ì¶”ì¶œ"""
    contents = get_contents(url, selector)
    return CrawlResponse(**contents)



'''

./app/routes/news.py
'''
from fastapi import APIRouter, HTTPException
import boto3
from boto3.dynamodb.conditions import Attr

from fastapi.responses import Response
from datetime import datetime, timedelta, timezone
import xml.etree.ElementTree as ET

router = APIRouter(
    prefix="/news",
    tags=["News Articles"]
)

# âœ… us-east-1 ë¦¬ì „ DynamoDB (í˜„ì¬ êµ¬ì¡°)
dynamodb = boto3.resource("dynamodb", region_name="us-east-1")
news_table = dynamodb.Table("NewsTable")


@router.get("/category/{category}")
def get_articles_by_category(category: str):
    """
    âœ… ì¹´í…Œê³ ë¦¬ë³„ ë‰´ìŠ¤ ëª©ë¡ (pubDate ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬)
    """
    try:
        # GSIê°€ ì—†ê¸° ë•Œë¬¸ì— scan + filter ì‚¬ìš©
        response = news_table.scan(
            FilterExpression=Attr("category").eq(category)
        )
        items = response.get("Items", [])

        # pubDate ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
        items.sort(key=lambda x: x.get("pubDate", ""), reverse=True)

        return items
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/article/{article_id}")
def get_article_detail(article_id: str):
    """
    âœ… ë‹¨ì¼ ë‰´ìŠ¤ ìƒì„¸ ì¡°íšŒ
    """
    try:
        response = news_table.get_item(Key={"articleId": article_id})
        if "Item" not in response:
            raise HTTPException(status_code=404, detail=f"Article not found: {article_id}")
        return response["Item"]
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

'''

./app/routes/name_map.py
'''
from fastapi import APIRouter, HTTPException, Body
from pydantic import BaseModel
from app.modules.name_mapper import (
    load_name_map_text,
    append_name_entry,
    overwrite_name_map,
    delete_name_entry,
)

router = APIRouter(prefix="/name-map", tags=["Name Mapping"])

class NameEntry(BaseModel):
    korean: str
    english: str
    description: str = ""


@router.get("")
def get_all_lines():
    """ì „ì²´ name_map.txt ì›ë¬¸ ë°˜í™˜"""
    text = load_name_map_text()
    return {"content": text}


@router.post("")
def add_new_entry(entry: NameEntry):
    """ìƒˆ í•­ëª© ì¶”ê°€ (ì¤‘ë³µ í—ˆìš©)"""
    line = f"{entry.korean}|{entry.english}|{entry.description}"
    append_name_entry(line)
    return {"message": "ì¶”ê°€ ì™„ë£Œ", "entry": line}


@router.put("")
def overwrite_whole_file(content: str = Body(..., embed=False)):
    """ì „ì²´ íŒŒì¼ ë®ì–´ì“°ê¸°"""
    if not content.strip():
        raise HTTPException(status_code=400, detail="ë‚´ìš©ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
    overwrite_name_map(content)
    return {"message": "íŒŒì¼ ì „ì²´ ë®ì–´ì“°ê¸° ì™„ë£Œ"}



@router.delete("/{korean_name}")
def delete_entry(korean_name: str):
    """í•œê¸€ëª… ê¸°ì¤€ìœ¼ë¡œ í•´ë‹¹ ë¼ì¸ ì‚­ì œ"""
    try:
        delete_name_entry(korean_name)
        return {"message": f"'{korean_name}' ê´€ë ¨ í•­ëª© ì‚­ì œ ì™„ë£Œ"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

'''

./app/routes/source.py
'''
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
import boto3
from boto3.dynamodb.conditions import Attr
import uuid

router = APIRouter(prefix="/sources", tags=["Sources"])

# DynamoDB
region = "us-east-1"
dynamodb = boto3.resource("dynamodb", region_name=region)
source_table = dynamodb.Table("SourceMetaTable")
article_table = dynamodb.Table("ArticleTable")


# -------------------------------
# âœ… Pydantic ëª¨ë¸
# -------------------------------
class SourceBase(BaseModel):
    srcName: str
    srcDescription: str
    sourceUrl: str
    selectorContainer: str
    selectorItem: str
    contentSelector: str  # âœ… ì¶”ê°€ë¨
    category: str


class SourceUpdate(SourceBase):
    pass


# -------------------------------
# âœ… API êµ¬í˜„
# -------------------------------

@router.get("")
def get_all_sources():
    """ëª¨ë“  ìˆ˜ì§‘ì²˜ ëª©ë¡ ì¡°íšŒ"""
    try:
        res = source_table.scan()
        return {"count": len(res["Items"]), "items": res["Items"]}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/{source_id}")
def get_source(source_id: str):
    """ë‹¨ì¼ ìˆ˜ì§‘ì²˜ ì¡°íšŒ"""
    try:
        res = source_table.get_item(Key={"sourceId": source_id})
        if "Item" not in res:
            raise HTTPException(status_code=404, detail="Source not found")
        return res["Item"]
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.post("")
def create_source(src: SourceBase):
    """ìƒˆë¡œìš´ ìˆ˜ì§‘ì²˜ ì¶”ê°€"""
    try:
        source_id = f"SRC-{uuid.uuid4().hex[:8]}"

        item = {
            "sourceId": source_id,
            "srcName": src.srcName,
            "srcDescription": src.srcDescription,
            "sourceUrl": src.sourceUrl,
            "selectorContainer": src.selectorContainer,
            "selectorItem": src.selectorItem,
            "contentSelector": src.contentSelector,  # âœ… ì¶”ê°€ë¨
            "category": src.category,
        }

        source_table.put_item(Item=item)
        return {"message": "Created successfully", "sourceId": source_id}

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.put("/{source_id}")
def update_source(source_id: str, data: SourceUpdate):
    """ê¸°ì¡´ ìˆ˜ì§‘ì²˜ ì •ë³´ ìˆ˜ì •"""
    try:
        update_expr = """
        SET srcName=:n,
            srcDescription=:d,
            sourceUrl=:u,
            selectorContainer=:c,
            selectorItem=:i,
            contentSelector=:s,
            category=:g
        """

        values = {
            ":n": data.srcName,
            ":d": data.srcDescription,
            ":u": data.sourceUrl,
            ":c": data.selectorContainer,
            ":i": data.selectorItem,
            ":s": data.contentSelector,  # âœ… ì¶”ê°€ë¨
            ":g": data.category,
        }

        source_table.update_item(
            Key={"sourceId": source_id},
            UpdateExpression=update_expr,
            ExpressionAttributeValues=values,
        )

        return {"message": "Updated successfully", "sourceId": source_id}

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.delete("/{source_id}")
def delete_source(source_id: str):
    """ìˆ˜ì§‘ì²˜ ì‚­ì œ"""
    try:
        source_table.delete_item(Key={"sourceId": source_id})
        return {"message": "Deleted successfully", "sourceId": source_id}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/{source_id}/articles")
def get_articles_by_source(source_id: str):
    """íŠ¹ì • ìˆ˜ì§‘ì²˜ì˜ ê¸°ì‚¬ ëª©ë¡ ì¡°íšŒ"""
    try:
        res = article_table.scan(FilterExpression=Attr("sourceId").eq(source_id))
        items = res.get("Items", [])
        return {"count": len(items), "items": items}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

'''

./app/routes/scrap.py
'''
from fastapi import APIRouter, HTTPException
from datetime import datetime
import boto3
import uuid
import traceback
from app.modules.crawling import extract_links, get_contents

router = APIRouter(prefix="/scrap", tags=["Scraper"])

# DynamoDB
region = "us-east-1"
dynamodb = boto3.resource("dynamodb", region_name=region)
source_table = dynamodb.Table("SourceMetaTable")
article_table = dynamodb.Table("ArticleTable")
lock_table = dynamodb.Table("ScrapLockTable")  # âœ… ë½ìš© í…Œì´ë¸” ì¶”ê°€ (PK: "scrap-lock")


@router.post("/run")
def run_scraper():
    """
    âœ… ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§í˜• ìë™ ìˆ˜ì§‘ê¸° (with DynamoDB Lock)
    - SourceMetaTable ê¸°ì¤€ìœ¼ë¡œ ê° ìˆ˜ì§‘ì²˜ 1íšŒ ìŠ¤ìº”
    - ëª©ë¡ selector / ë³¸ë¬¸ selector ë‘˜ ë‹¤ í…Œì´ë¸”ì—ì„œ ì§€ì •
    - ì´ë¯¸ ë“±ë¡ëœ URLì€ ì œì™¸
    - ì‹ ê·œ ê¸°ì‚¬ë§Œ ArticleTableì— ì €ì¥
    - í˜ì´ì§• ì—†ìŒ
    - ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€ (DynamoDB Lock)
    """

    # âœ… 1. ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸
    try:
        lock_item = lock_table.get_item(Key={"PK": "scrap-lock"}).get("Item")
        if lock_item and lock_item.get("isRunning"):
            raise HTTPException(status_code=409, detail="Scraper already running")

        # âœ… 2. ë½ ì„¤ì •
        lock_table.put_item(
            Item={
                "PK": "scrap-lock",
                "isRunning": True,
                "startedAt": datetime.utcnow().isoformat(),
            }
        )

        print("ğŸš€ ìˆ˜ì§‘ê¸° ì‹¤í–‰ ì‹œì‘")

        # âœ… 3. ì‹¤ì œ ìˆ˜ì§‘ ë¡œì§
        res = source_table.scan()
        sources = res.get("Items", [])
        if not sources:
            raise HTTPException(status_code=404, detail="No sources found")

        total_new = 0
        total_skipped = 0
        total_failed = 0
        result_summary = []

        for src in sources:
            src_id = src["sourceId"]
            src_name = src["srcName"]
            base_url = src["sourceUrl"]
            selector_container = src.get("selectorContainer")
            selector_item = src.get("selectorItem", "a")
            selector_content = src.get("contentSelector")
            category = src.get("category", "General")

            print(f"ğŸ•·ï¸ {src_name} ({src_id}) â†’ {base_url}")

            try:
                links = extract_links(base_url, selector_container, selector_item)
            except Exception as e:
                print(f"âš ï¸ [{src_name}] ë§í¬ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                total_failed += 1
                continue

            new_count = 0
            skip_count = 0
            fail_count = 0

            for link in links:
                # URL ì •ê·œí™”
                if link.startswith("/"):
                    full_url = base_url.rstrip("/") + link
                elif link.startswith("http"):
                    full_url = link
                else:
                    full_url = f"{base_url.rstrip('/')}/{link}"

                # ì¤‘ë³µ í™•ì¸
                exists = article_table.scan(
                    FilterExpression="articleUrl = :u",
                    ExpressionAttributeValues={":u": full_url}
                )
                if exists.get("Items"):
                    skip_count += 1
                    continue

                try:
                    # âœ… ë³¸ë¬¸ selectorë¥¼ ë™ì ìœ¼ë¡œ ì „ë‹¬
                    data = get_contents(full_url, selector_content)
                    html = data.get("html", "")
                    imgs = data.get("images", [])
                    image_url = imgs[0]["src"] if imgs else None

                    article_id = f"{src_id}-{uuid.uuid4().hex[:8]}"

                    article_table.put_item(
                        Item={
                            "articleId": article_id,
                            "sourceId": src_id,
                            "articleUrl": full_url,
                            "content": html,
                            "imageUrl": image_url,
                            "date": datetime.utcnow().isoformat(),
                            "category": category,
                            "contentSelector": selector_content,
                        }
                    )

                    new_count += 1
                    total_new += 1

                except Exception as e:
                    print(f"âš ï¸ [{src_name}] {full_url} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                    fail_count += 1
                    total_failed += 1

            result_summary.append({
                "sourceId": src_id,
                "sourceName": src_name,
                "checkedLinks": len(links),
                "newArticles": new_count,
                "skipped": skip_count,
                "failed": fail_count,
            })
            total_skipped += skip_count

        return {
            "status": "ok",
            "timestamp": datetime.utcnow().isoformat(),
            "totalNew": total_new,
            "totalSkipped": total_skipped,
            "totalFailed": total_failed,
            "summary": result_summary,
        }

    except HTTPException:
        raise
    except Exception as e:
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))

    finally:
        # âœ… 4. ë½ í•´ì œ (ì˜ˆì™¸ ë°œìƒ ì—¬ë¶€ì™€ ìƒê´€ì—†ì´)
        try:
            lock_table.put_item(
                Item={
                    "PK": "scrap-lock",
                    "isRunning": False,
                    "finishedAt": datetime.utcnow().isoformat(),
                }
            )
            print("âœ… ë½ í•´ì œ ì™„ë£Œ")
        except Exception as unlock_err:
            print(f"âš ï¸ ë½ í•´ì œ ì‹¤íŒ¨: {unlock_err}")

'''

./app/routes/articles.py
'''
from fastapi import APIRouter, HTTPException
import boto3
import uuid
import requests
import re
from app.modules.bedrock import call_bedrock_api, parse_bedrock_output
from app.modules.prompt_loader import load_prompt  
from app.modules.name_mapper import load_name_map_text

from fastapi.responses import Response
from datetime import datetime, timedelta, timezone
import xml.etree.ElementTree as ET

router = APIRouter(prefix="/articles", tags=["Articles"])

# âœ… AWS ë¦¬ì†ŒìŠ¤
region = "us-east-1"
dynamodb = boto3.resource("dynamodb", region_name=region)
s3 = boto3.client("s3", region_name=region)

article_table = dynamodb.Table("ArticleTable")
news_table = dynamodb.Table("NewsTable")

# âœ… ì—…ë¡œë“œí•  S3 ë²„í‚·ëª…
TARGET_BUCKET = "sayart-news-thumbnails"


@router.post("/generate-news/{article_id}")
def generate_news_from_article(article_id: str):
    """ê¸°ì‚¬ ê¸°ë°˜ìœ¼ë¡œ ë‰´ìŠ¤ ìƒì„± + ì´ë¯¸ì§€ S3 ì—…ë¡œë“œ í›„ ë³¸ë¬¸ ì‚½ì…"""
    try:
        print(article_id)
        res = article_table.get_item(Key={"articleId": article_id})
        if "Item" not in res:
            raise HTTPException(status_code=404, detail="Article not found")
        article = res["Item"]

        # ì´ë¯¸ ìƒì„±ëœ ë‰´ìŠ¤ê°€ ìˆìœ¼ë©´ ì¤‘ë³µ ë°©ì§€
        if article.get("generatedNewsId"):
            return {"message": "Already generated", "newsId": article["generatedNewsId"]}

        category = article.get("category")
        if not category:
            raise HTTPException(status_code=400, detail="Missing category info")

        image_url = article.get("imageUrl")


        # í”„ë¡¬í”„íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸°
        try:
            template = load_prompt("generate_news") 
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"Prompt load failed: {e}")


        name_map_text = load_name_map_text()
        # âœ… í…œí”Œë¦¿ ë³€ìˆ˜ ì¹˜í™˜
        prompt = (
            template
            .replace("{{content}}", article.get("content", ""))
            .replace("{{image_url}}", image_url or "")
            .replace("{{article_url}}", article.get("articleUrl", ""))
            .replace("{{name_map}}", name_map_text)
        )

        # âœ… Bedrock í˜¸ì¶œ
        result = call_bedrock_api(prompt=prompt, model_name="haiku-3.5")
        text = result["content"][0]["text"] if "content" in result else str(result)
        title, description = parse_bedrock_output(text)
        
        

        if not title or not description:
            raise HTTPException(status_code=500, detail="Bedrock output parsing failed")
        
        description = re.sub(r'\n{2,}', '</p><p>', description.strip())
        description = f"<p>{description}</p>"


        # âœ… ìƒˆ ë‰´ìŠ¤ ìƒì„±
        new_id = str(uuid.uuid4())[:8]
        now = datetime.utcnow().isoformat()

        news_table.put_item(
            Item={
                "articleId": new_id,
                "title": title,
                "description": description,
                "sourceArticleId": article_id,
                "category": category,
                "pubDate": now,
                "author": "System",
                "imageUrl": image_url,
            }
        )

        # âœ… ArticleTableì— generatedNewsId ì—…ë°ì´íŠ¸
        article_table.update_item(
            Key={"articleId": article_id},
            UpdateExpression="SET generatedNewsId = :nid",
            ExpressionAttributeValues={":nid": new_id},
        )

        return {
            "message": "Generated successfully",
            "id": new_id,
            "title": title,
            "description": description,
            "category": category,
            "imageUrl": image_url,
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/generate-batch")
def generate_all_unprocessed_articles():
    """
    ì•„ì§ ë‰´ìŠ¤ê°€ ìƒì„±ë˜ì§€ ì•Šì€ ê¸°ì‚¬ë“¤(generateFlag=0)ì„ ëª¨ë‘ ìƒì„±
    """
    try:
        # 1ï¸âƒ£ generateFlag == 0 ì¸ ê¸°ì‚¬ ëª©ë¡ ì¡°íšŒ
        res = article_table.scan(
            FilterExpression="attribute_not_exists(generateFlag) OR generateFlag = :flag",
            ExpressionAttributeValues={":flag": 0}
        )
        articles = res.get("Items", [])
        if not articles:
            return {"message": "ìƒì„±í•  ì‹ ê·œ ê¸°ì‚¬ ì—†ìŒ", "count": 0}

        total_success = 0
        total_fail = 0
        results = []

        for article in articles:
            article_id = article["articleId"]

            try:
                # ê¸°ì¡´ ë‹¨ì¼ ìƒì„± ë¡œì§ ì¬ì‚¬ìš©
                sub_res = generate_news_from_article(article_id)

                # ì„±ê³µ ì‹œ í”Œë˜ê·¸ 1ë¡œ ì—…ë°ì´íŠ¸
                article_table.update_item(
                    Key={"articleId": article_id},
                    UpdateExpression="SET generateFlag = :f, generateError = :e",
                    ExpressionAttributeValues={
                        ":f": 1,
                        ":e": "SUCCESS"
                    }
                )

                total_success += 1
                results.append({"articleId": article_id, "status": "âœ… success"})

            except Exception as e:
                # ì‹¤íŒ¨ ì‹œ í”Œë˜ê·¸ 2 ë° ì˜¤ë¥˜ë‚´ìš© ê¸°ë¡
                article_table.update_item(
                    Key={"articleId": article_id},
                    UpdateExpression="SET generateFlag = :f, generateError = :e",
                    ExpressionAttributeValues={
                        ":f": 2,
                        ":e": str(e)
                    }
                )

                total_fail += 1
                results.append({"articleId": article_id, "status": f"âŒ failed: {e}"})

        return {
            "message": "Batch generation completed",
            "totalSuccess": total_success,
            "totalFail": total_fail,
            "processed": len(articles),
            "results": results
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/rss/generated")
def generate_and_upload_rss_to_s3():
    """
    âœ… ì˜¤ëŠ˜ ìƒì„±ëœ ë‰´ìŠ¤ ê¸°ë°˜ RSS XML ìƒì„± â†’ S3 ì—…ë¡œë“œ (í¼ë¸”ë¦­)
    (xml.etree.ElementTree â†’ xml.dom.minidom ê¸°ë°˜ìœ¼ë¡œ êµì²´í•˜ì—¬ ì§„ì§œ CDATA ì ìš©)
    """
    try:
        from xml.dom.minidom import Document

        KST = timezone(timedelta(hours=9))
        now_kst = datetime.now(KST)
        today_kst_str = now_kst.strftime("%Y-%m-%d")

        # 1ï¸âƒ£ DynamoDB ë‰´ìŠ¤ ìŠ¤ìº”
        res = news_table.scan()
        items = res.get("Items", [])

        # 2ï¸âƒ£ ì˜¤ëŠ˜ ìƒì„±ëœ ë‰´ìŠ¤ë§Œ í•„í„°ë§
        recent_items = []
        for item in items:
            pub_date_str = item.get("pubDate", "")
            if not pub_date_str:
                continue
            try:
                pub_dt = datetime.fromisoformat(pub_date_str.replace("Z", "+00:00")).astimezone(KST)
                if pub_dt.strftime("%Y-%m-%d") == today_kst_str:
                    recent_items.append(item)
            except Exception:
                continue

        # 3ï¸âƒ£ ìµœì‹ ìˆœ ì •ë ¬ + ìµœëŒ€ 100ê°œ
        recent_items.sort(key=lambda x: x.get("pubDate", ""), reverse=True)
        recent_items = recent_items[:100]

        # 4ï¸âƒ£ DOM ê¸°ë°˜ RSS XML ìƒì„±
        doc = Document()

        rss = doc.createElement("rss")
        rss.setAttribute("xmlns:atom", "http://www.w3.org/2005/Atom")
        rss.setAttribute("xmlns:art", "http://artnews.local/rss")
        rss.setAttribute("version", "2.0")
        doc.appendChild(rss)

        # script = doc.createElement("script")
        # rss.appendChild(script)

        channel = doc.createElement("channel")
        rss.appendChild(channel)

        def add_text(tag, text):
            el = doc.createElement(tag)
            el.appendChild(doc.createTextNode(text))
            channel.appendChild(el)
            return el

        add_text("title", "ArtNews Recent Articles")
        add_text("link", "http://cc.xxq.me/art_news/rss.xml")
        add_text("description", "ì˜¤ëŠ˜ ìƒì„±ëœ ì•„íŠ¸ ê¸°ì‚¬ ëª©ë¡")
        add_text("language", "ko")

        pub_str = now_kst.strftime("%a, %d %b %Y %H:%M:%S +0900")
        add_text("pubDate", pub_str)
        add_text("lastBuildDate", pub_str)

        atom_link = doc.createElement("atom:link")
        atom_link.setAttribute("href", "http://cc.xxq.me/art_news/rss.xml")
        atom_link.setAttribute("rel", "self")
        atom_link.setAttribute("type", "application/rss+xml")
        channel.appendChild(atom_link)


        byline = '''\n\nSayArt / Sayart Teams'''


        # 5ï¸âƒ£ ì•„ì´í…œ ë£¨í”„
        for item in recent_items:
            item_el = doc.createElement("item")
            channel.appendChild(item_el)

            # ì§„ì§œ CDATA ë¸”ë¡ ìƒì„±
            def add_cdata(tag, text):
                el = doc.createElement(tag)
                el.appendChild(doc.createCDATASection(text))
                item_el.appendChild(el)

            add_cdata("title", item.get("title", "Untitled"))
            link_el = doc.createElement("link")
            link_el.appendChild(doc.createTextNode(item.get("articleUrl", "")))
            item_el.appendChild(link_el)
            add_cdata("description", item.get("description", "") + byline)
            add_cdata("category", item.get("category", "general"))

            # articleId (namespace í¬í•¨)
            art_id = doc.createElement("art:articleId")
            art_id.appendChild(doc.createTextNode(str(item.get("articleId", ""))))
            item_el.appendChild(art_id)

            # imageUrl (ì²« ë²ˆì§¸ ì´ë¯¸ì§€ë§Œ)
            if item.get("imageUrl"):
                img_el = doc.createElement("imageUrl")
                img_el.appendChild(doc.createTextNode(item["imageUrl"]))
                item_el.appendChild(img_el)

            # pubDate (RFC í˜•ì‹ ë³€í™˜)
            pub_dt = datetime.fromisoformat(
                item.get("pubDate", now_kst.isoformat())
            ).astimezone(KST)
            pub_date_str = pub_dt.strftime("%a, %d %b %Y %H:%M:%S +0900")
            pub_el = doc.createElement("pubDate")
            pub_el.appendChild(doc.createTextNode(pub_date_str))
            item_el.appendChild(pub_el)

        # 6ï¸âƒ£ XML ë¬¸ìì—´ ì§ë ¬í™” (UTF-8)
        xml_bytes = doc.toprettyxml(indent="  ", encoding="utf-8")
        
        # BOM ì¶”ê°€
        BOM = b'\xef\xbb\xbf'
        xml_bytes = BOM + xml_bytes

        # 7ï¸âƒ£ S3 ì—…ë¡œë“œ (í¼ë¸”ë¦­)
        file_name = f"rss/ArtNews_{today_kst_str}.xml"
        s3.put_object(
            Bucket=TARGET_BUCKET,
            Key=file_name,
            Body=xml_bytes,
            ContentType="application/rss+xml; charset=utf-8",
        )

        public_url = f"https://{TARGET_BUCKET}.s3.amazonaws.com/{file_name}"

        # âœ… ë°˜í™˜: RSS ì—…ë¡œë“œ ì •ë³´ë§Œ
        return {
            "message": "RSS generated and uploaded successfully",
            "itemCount": len(recent_items),
            "rssFile": file_name,
            "rssUrl": public_url,
            "generatedAt": pub_str,
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

'''


from fastapi import APIRouter, HTTPException
from datetime import datetime
import boto3
import uuid
import traceback
from app.modules.crawling import extract_links, get_contents

router = APIRouter(prefix="/scrap", tags=["Scraper"])

# DynamoDB
region = "us-east-1"
dynamodb = boto3.resource("dynamodb", region_name=region)
source_table = dynamodb.Table("SourceMetaTable")
article_table = dynamodb.Table("ArticleTable")
lock_table = dynamodb.Table("ScrapLockTable")  # âœ… ë½ìš© í…Œì´ë¸” ì¶”ê°€ (PK: "scrap-lock")


@router.post("/run")
def run_scraper():
    """
    âœ… ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§í˜• ìë™ ìˆ˜ì§‘ê¸° (with DynamoDB Lock)
    - SourceMetaTable ê¸°ì¤€ìœ¼ë¡œ ê° ìˆ˜ì§‘ì²˜ 1íšŒ ìŠ¤ìº”
    - ëª©ë¡ selector / ë³¸ë¬¸ selector ë‘˜ ë‹¤ í…Œì´ë¸”ì—ì„œ ì§€ì •
    - ì´ë¯¸ ë“±ë¡ëœ URLì€ ì œì™¸
    - ì‹ ê·œ ê¸°ì‚¬ë§Œ ArticleTableì— ì €ì¥
    - í˜ì´ì§• ì—†ìŒ
    - ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€ (DynamoDB Lock)
    """

    # âœ… 1. ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸
    try:
        lock_item = lock_table.get_item(Key={"PK": "scrap-lock"}).get("Item")
        if lock_item and lock_item.get("isRunning"):
            raise HTTPException(status_code=409, detail="Scraper already running")

        # âœ… 2. ë½ ì„¤ì •
        lock_table.put_item(
            Item={
                "PK": "scrap-lock",
                "isRunning": True,
                "startedAt": datetime.utcnow().isoformat(),
            }
        )

        print("ğŸš€ ìˆ˜ì§‘ê¸° ì‹¤í–‰ ì‹œì‘")

        # âœ… 3. ì‹¤ì œ ìˆ˜ì§‘ ë¡œì§
        res = source_table.scan()
        sources = res.get("Items", [])
        if not sources:
            raise HTTPException(status_code=404, detail="No sources found")

        total_new = 0
        total_skipped = 0
        total_failed = 0
        result_summary = []

        for src in sources:
            src_id = src["sourceId"]
            src_name = src["srcName"]
            base_url = src["sourceUrl"]
            selector_container = src.get("selectorContainer")
            selector_item = src.get("selectorItem", "a")
            selector_content = src.get("contentSelector")
            category = src.get("category", "General")

            print(f"ğŸ•·ï¸ {src_name} ({src_id}) â†’ {base_url}")

            try:
                links = extract_links(base_url, selector_container, selector_item)
            except Exception as e:
                print(f"âš ï¸ [{src_name}] ë§í¬ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                total_failed += 1
                continue

            new_count = 0
            skip_count = 0
            fail_count = 0

            for link in links:
                # URL ì •ê·œí™”
                if link.startswith("/"):
                    full_url = base_url.rstrip("/") + link
                elif link.startswith("http"):
                    full_url = link
                else:
                    full_url = f"{base_url.rstrip('/')}/{link}"

                # ì¤‘ë³µ í™•ì¸
                exists = article_table.scan(
                    FilterExpression="articleUrl = :u",
                    ExpressionAttributeValues={":u": full_url}
                )
                if exists.get("Items"):
                    skip_count += 1
                    continue

                try:
                    # âœ… ë³¸ë¬¸ selectorë¥¼ ë™ì ìœ¼ë¡œ ì „ë‹¬
                    data = get_contents(full_url, selector_content)
                    html = data.get("html", "")
                    imgs = data.get("images", [])
                    image_url = imgs[0]["src"] if imgs else None

                    article_id = f"{src_id}-{uuid.uuid4().hex[:10]}"

                    article_table.put_item(
                        Item={
                            "articleId": article_id,
                            "sourceId": src_id,
                            "articleUrl": full_url,
                            "content": html,
                            "imageUrl": image_url,
                            "date": datetime.utcnow().isoformat(),
                            "category": category,
                            "contentSelector": selector_content,
                        }
                    )

                    new_count += 1
                    total_new += 1

                except Exception as e:
                    print(f"âš ï¸ [{src_name}] {full_url} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                    fail_count += 1
                    total_failed += 1

            result_summary.append({
                "sourceId": src_id,
                "sourceName": src_name,
                "checkedLinks": len(links),
                "newArticles": new_count,
                "skipped": skip_count,
                "failed": fail_count,
            })
            total_skipped += skip_count

        return {
            "status": "ok",
            "timestamp": datetime.utcnow().isoformat(),
            "totalNew": total_new,
            "totalSkipped": total_skipped,
            "totalFailed": total_failed,
            "summary": result_summary,
        }

    except HTTPException:
        raise
    except Exception as e:
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))

    finally:
        # âœ… 4. ë½ í•´ì œ (ì˜ˆì™¸ ë°œìƒ ì—¬ë¶€ì™€ ìƒê´€ì—†ì´)
        try:
            lock_table.put_item(
                Item={
                    "PK": "scrap-lock",
                    "isRunning": False,
                    "finishedAt": datetime.utcnow().isoformat(),
                }
            )
            print("âœ… ë½ í•´ì œ ì™„ë£Œ")
        except Exception as unlock_err:
            print(f"âš ï¸ ë½ í•´ì œ ì‹¤íŒ¨: {unlock_err}")
